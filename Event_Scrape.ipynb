{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from html import unescape\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import lxml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Load & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv('preprocessed_data.csv') #DF of matches, each observation is a match between two fighters\n",
    "fighter_stats = pd.read_csv('data.csv') #fighter's cumulative statistics database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_cols = list(processed_df.columns) #all columns in preprocessed data\n",
    "fighter_cols = list(fighter_stats.columns) #all columns in the fighter stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a sublist of strings that begin with letter from a list of strings\n",
    "def select_cols_start(letter, list_str):\n",
    "    pattern = re.compile(r'{}.*'.format(letter))\n",
    "    col_matches = list(filter(None, [re.match(pattern, item) for item in list_str]))\n",
    "    cols = [item.group(0) for item in col_matches]\n",
    "    \n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_match_cols = select_cols_start('R_', processed_cols)\n",
    "b_match_cols = select_cols_start('B_', processed_cols)\n",
    "\n",
    "r_fighter_cols = select_cols_start('R_', fighter_cols)\n",
    "b_fighter_cols = select_cols_start('B_', fighter_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup URL Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://ufcstats.com/statistics/events/completed'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of more urls to scrape:\n",
    "detail_urls = [] #this grabs urls for each day recorded\n",
    "for url in soup.find_all(class_='b-link b-link_style_black'):\n",
    "    detail_urls.append(url['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://ufcstats.com/event-details/0b5b6876c2a4723f'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detail_urls[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Fight Day Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = {\n",
    "    'R_STR': int, \n",
    "    'B_STR': int,\n",
    "    'R_TD': int, \n",
    "    'B_TD': int, \n",
    "    'R_SUB': int, \n",
    "    'R_SUB': int, \n",
    "    'R_PASS': int, \n",
    "    'B_PASS': int,\n",
    "    'ROUND': int,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space_lines(text):\n",
    "    pattern1 = re.compile(r'[\\s\\s+]')\n",
    "    return re.sub(pattern1, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine if observation is a title-bout\n",
    "def find_belt(img_tag):\n",
    "    try:\n",
    "        image_link = img_tag['src']\n",
    "        if re.match(r'.*belt.*', image_link) != None:\n",
    "            return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fight_auxiliary(soup):\n",
    "    \n",
    "    table = []\n",
    "    \n",
    "    auxiliary_table = soup.find_all('li', {'class': 'b-list__box-list-item'})\n",
    "    for item in auxiliary_table:\n",
    "        attribute = remove_space_lines(item.text).strip()\n",
    "\n",
    "        try:\n",
    "            attribute = re.findall(r'\\s\\s+(.*)', attribute)[0]\n",
    "        except:\n",
    "            attribute = '' \n",
    "        \n",
    "        table.append(attribute)\n",
    "        \n",
    "    table_series = pd.Series(table)\n",
    "    table_series.index = ['date', 'location', 'attendance']\n",
    "    \n",
    "    if table_series['attendance'] != '':\n",
    "        table_series['attendance'] = re.sub(',', '', table_series['attendance'])\n",
    "        table_series['attendance'] = int(table_series['attendance'])\n",
    "    \n",
    "    table_series['date'] = dt.strptime(table_series['date'], '%B %d, %Y').strftime('%d-%m-%Y')\n",
    "\n",
    "    return table_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_stats(url):\n",
    "    \n",
    "    #Given url of list of events, returns list of event details:\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    stat_table = soup.findAll('table')[0].contents #Contents of the main table in html\n",
    "    \n",
    "    table_data = stat_table[3] #first 2 indices are empty strings, table_data is html starting from first table row\n",
    "    detail_data = table_data.find_all('p') #within table rows, there are <p> labels for table text\n",
    "    auxiliary_data = get_fight_auxiliary(soup)\n",
    "    \n",
    "    image_data = table_data.find_all('img') #get image links to find belt for \n",
    "    \n",
    "    contents = [] #table contents\n",
    "    title_match_index = [] #track which fights are title_bouts\n",
    "    \n",
    "    #Loop through elements of detail_data (html table) to scrape fight details:\n",
    "    for index, item in enumerate(detail_data):\n",
    "        \n",
    "        #find image of belt == title_bout\n",
    "        image = item.find('img')\n",
    "        if find_belt(image):\n",
    "            title_match_index.append(index) \n",
    "            \n",
    "        #contents is list of all text from each element of table     \n",
    "        contents.append(item.text)  \n",
    "\n",
    "    \n",
    "    #Clean up elements\n",
    "    contents = list(map(lambda x: remove_space_lines(x), contents))\n",
    "    contents = list(map(lambda x: x.strip(), contents)) \n",
    "    \n",
    "    draw_index = []\n",
    "    \n",
    "    #When there's a draw or NC, additional tags are created --> remove the tag to reformat correctly   \n",
    "    for i in np.arange(0, len(contents)-10, 16):\n",
    "\n",
    "        if contents[i] != 'win':\n",
    "            \n",
    "            #Get the index of the match that was drawn & remove that element\n",
    "            draw_index.append(np.floor_divide(i, 16)) \n",
    "            contents.pop(i)\n",
    "                    \n",
    "    #Extract links to more detailed fight statistics\n",
    "    fight_links = table_data.find_all('a', {'class': 'b-flag b-flag_style_green'})\n",
    "    fight_links = [item['href'] for item in fight_links]\n",
    "    \n",
    "    draw_links = table_data.find_all('a', {'class': 'b-flag b-flag_style_bordered'})\n",
    "    draw_links = [item['href'] for item in draw_links]\n",
    "    draw_links = list(dict.fromkeys(draw_links)) #Remove duplicate links from the drawn fights\n",
    "    \n",
    "    for index, link in zip(draw_index, draw_links):\n",
    "        fight_links.insert(index, link)\n",
    "    \n",
    "    #each row of data is 16 elements: reformats 1 observation per row\n",
    "    formatted_contents = np.array(contents).reshape((-1, 16))\n",
    "    formatted_contents = pd.DataFrame(formatted_contents)\n",
    "    \n",
    "    #the first row is a list of 'wins'\n",
    "    #formatted_contents.drop(0, axis = 1, inplace = True)\n",
    "    \n",
    "    #Run a floor_divide to put the image of the belt in the correct fight\n",
    "\n",
    "    title_match = np.floor_divide(title_match_index, 16) \n",
    "\n",
    "    #Initialize title_bout column with all 0's\n",
    "    titles = np.zeros(16)\n",
    "    if len(title_match) != 0:\n",
    "        titles[title_match] = 1\n",
    "    \n",
    "    title_series = pd.Series(titles)\n",
    "    \n",
    "    formatted_contents['title_bout'] = title_series\n",
    "    \n",
    "    #rename columns\n",
    "    formatted_contents.columns = ['Winner', 'R_fighter', 'B_fighter', 'R_STR', 'B_STR', \n",
    "                               'R_TD', 'B_TD', 'R_SUB', 'R_SUB', 'R_PASS', 'B_PASS',\n",
    "                              'WEIGHT_CLASS', 'METHOD', 'DETAIL', 'ROUND', 'TIME', 'title_bout']\n",
    "    \n",
    "    #convert columns to appropriate data types\n",
    "    formatted_contents.replace('--', 99999, inplace = True)\n",
    "    formatted_contents = formatted_contents.astype(data_types)\n",
    "    formatted_contents['TIME'] = formatted_contents['TIME'].apply(lambda x: dt.strptime(x, '%M:%S').time())\n",
    "    formatted_contents['link'] = fight_links\n",
    "    \n",
    "    return (formatted_contents, auxiliary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Extract all fight day / event URLs from the main page\n",
    "    2. Extract all fights from a particular event / day | Input event url --> get_page_stats\n",
    "    3. Within each event, extract get_detailed_page_statistics | Input fight url --> get_detailed_page_stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-357ee2fa254d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mpage_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#all_event_urls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#STEP 1:\n",
    "base_url = 'http://ufcstats.com/statistics/events/completed'\n",
    "\n",
    "url = 'http://ufcstats.com/statistics/events/completed?page=22'\n",
    "\n",
    "all_event_urls = []\n",
    "page_index = 1\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    url = base_url + '?page={}'.format(page_index)\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    links = soup.find_all(class_='b-link b-link_style_black')\n",
    "    \n",
    "    if len(links) == 0:\n",
    "        break\n",
    "    \n",
    "    for item in links:\n",
    "        all_event_urls.append(item['href'])\n",
    "    \n",
    "    page_index += 1\n",
    "    \n",
    "    time.sleep(1)\n",
    "        \n",
    "#all_event_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class event_details(object):\n",
    "    def __init__(self, url):\n",
    "        self.all_details = get_page_stats(url)\n",
    "        self.fight_df = self.all_details[0]\n",
    "        self.attributes = self.all_details[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483\n",
      "488\n",
      "492\n",
      "493\n",
      "494\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n"
     ]
    }
   ],
   "source": [
    "all_events = []\n",
    "\n",
    "for index, url in enumerate(all_event_urls):\n",
    "    try:\n",
    "        all_events.append(get_page_stats(url))\n",
    "    except:\n",
    "        all_events.append(index)\n",
    "        print(index)\n",
    "        \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_index = [item for item in all_events if type(item) != tuple]\n",
    "error_urls = [all_event_urls[item] for item in error_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[483, 488, 492, 493, 494, 499, 500, 501, 502, 504, 505, 506, 507, 508, 509]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://ufcstats.com/event-details/32a3025d5db456ae'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_url = all_event_urls[error_index[2]]\n",
    "error_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483 http://ufcstats.com/event-details/cedfdf8d423d500c\n",
      "488 http://ufcstats.com/event-details/1a1a4d7a29041d77\n",
      "492 http://ufcstats.com/event-details/32a3025d5db456ae\n",
      "493 http://ufcstats.com/event-details/4a01dc8376736ef5\n",
      "494 http://ufcstats.com/event-details/749685d24e2cac50\n",
      "499 http://ufcstats.com/event-details/96eff1a628adcc7f\n",
      "500 http://ufcstats.com/event-details/9b5b5a75523728f3\n",
      "501 http://ufcstats.com/event-details/6ceff86fae4f6b3b\n",
      "502 http://ufcstats.com/event-details/aee8eecfc4bfb1e7\n",
      "504 http://ufcstats.com/event-details/b63e800c18e011b5\n",
      "505 http://ufcstats.com/event-details/31bbd46d57dfbcb7\n",
      "506 http://ufcstats.com/event-details/5af480a3b2e1726b\n",
      "507 http://ufcstats.com/event-details/1c3f5e85b59ec710\n",
      "508 http://ufcstats.com/event-details/dedc3bb440d09554\n",
      "509 http://ufcstats.com/event-details/b60391da771deefe\n"
     ]
    }
   ],
   "source": [
    "input_error_entries = []\n",
    "for index, url in zip(error_index, error_urls):\n",
    "    print(index, url)\n",
    "    input_error_entries.append(get_page_stats(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, data in zip(error_index, input_error_entries):\n",
    "    all_events[index] = data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in np.arange(0, len(all_events)):\n",
    "    all_events[index][0]['date'] = all_events[index][1].loc['date']\n",
    "    all_events[index][0]['location'] = all_events[index][1].loc['location']\n",
    "    all_events[index][0]['attendance'] = all_events[index][1].loc['attendance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_level_data = pd.DataFrame([])\n",
    "for item in all_events:\n",
    "    event_level_data = pd.concat([event_level_data, item[0]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_level_data.reset_index(drop=True, inplace=True)\n",
    "event_level_data.to_csv('event_level_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each fight in fight_day scrape detail statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_totals_table(soup):\n",
    "\n",
    "    html_table = soup.find_all('table')[0]\n",
    "    \n",
    "    #Scrape Statistics: separated by double-space\n",
    "    stats_table = pd.read_html(str(html_table))[0]\n",
    "    table_columns = stats_table.columns\n",
    "    total_statistics = [item.split('  ') for item in stats_table.loc[0][1:]]\n",
    "    \n",
    "    #Scrape Fighter names\n",
    "    names_table = soup.find('td', {'class': 'b-fight-details__table-col l-page_align_left'})    \n",
    "    names = [remove_space_lines(item.text).strip() for item in names_table.find_all('p')]\n",
    "    \n",
    "    #Append together, rearrange columns, and rename columns:\n",
    "    total_statistics.append(names)\n",
    "\n",
    "    total_statistics = pd.DataFrame(total_statistics).T\n",
    "    total_statistics = total_statistics[[9,0,1,2,3,4,5,6,7,8]]\n",
    "    total_statistics.columns = table_columns\n",
    "    \n",
    "    return total_statistics\n",
    "\n",
    "def get_ss_table(soup):\n",
    "    html_table = soup.find_all('table')[2]\n",
    "    \n",
    "    #Scrape Statistics: separated by double-space\n",
    "    stats_table = pd.read_html(str(html_table))[0]\n",
    "    table_columns = stats_table.columns\n",
    "    total_statistics = [item.split('  ') for item in stats_table.loc[0][1:]]\n",
    "    \n",
    "    #Append together, rearrange columns, and rename columns:\n",
    "    #total_statistics.append(names)\n",
    "\n",
    "    total_statistics = pd.DataFrame(total_statistics).T\n",
    "    total_statistics.columns = table_columns[1:]\n",
    "    \n",
    "    return total_statistics\n",
    "    \n",
    "def get_combined(soup):\n",
    "    name_stats = get_totals_table(soup)\n",
    "    significant_stats = get_ss_table(soup)\n",
    "    \n",
    "    combined = pd.concat([name_stats, significant_stats], axis = 1)\n",
    "    combined = combined.loc[:, ~combined.columns.duplicated()]\n",
    "    combined.drop('Sig. str', axis = 1, inplace = True)\n",
    "        \n",
    "    return combined\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Countables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "countables = ['Sig. str.', 'Total str.', 'Td', 'Head', 'Body', 'Leg', 'Distance', 'Clinch', 'Ground']\n",
    "hit_att = ['Hits', 'Attempts']\n",
    "\n",
    "countable_cols = list(itertools.product(countables, hit_att))\n",
    "countable_cols = ['{} {}'.format(item[0], item[1]) for item in countable_cols]\n",
    "#countable_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fight_attributes(soup):\n",
    "        \n",
    "    header_table = soup.find_all('i', {'class': 'b-fight-details__text-item'})[0:4]\n",
    "    \n",
    "    table = []\n",
    "    for item in header_table:\n",
    "\n",
    "        detail = remove_space_lines(item.text).strip()\n",
    "        \n",
    "        try:\n",
    "            table.append(re.findall(r'\\s\\s+(.*)', detail)[0])\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    table_series = pd.Series(table, index=['rounds', 'time', 'format', 'referee'])\n",
    "    #table_series = table_series.astype(data_types)\n",
    "    table_series['rounds'] = int(table_series['rounds'])\n",
    "    table_series['time'] = dt.strptime(table_series['time'], '%M:%S').time()\n",
    "        \n",
    "\n",
    "    return table_series\n",
    "\n",
    "def split_countable(combined_df):\n",
    "    split = combined_df.apply(lambda x: x.apply(lambda y: y.split('of')))\n",
    "    \n",
    "    split_stats = []\n",
    "    for index, series in split.iterrows():\n",
    "        split_stats.append(list(series.apply(pd.Series).stack()))\n",
    "        \n",
    "    split_df = pd.DataFrame(split_stats)\n",
    "    split_df.columns = countable_cols\n",
    "    \n",
    "    return split_df\n",
    "    \n",
    "def get_detailed_page_stats(url):\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    #get Referee:\n",
    "    attributes = fight_attributes(soup)\n",
    "    \n",
    "    #Get fight_df:\n",
    "    combined_df = get_combined(soup)\n",
    "    split_df = combined_df[countables]\n",
    "    \n",
    "    combined_df.drop(countables, axis = 1, inplace = True)\n",
    "    \n",
    "    countable_df = split_countable(split_df)\n",
    "    \n",
    "    combined_df = pd.concat([combined_df, countable_df], axis = 1)\n",
    "    return (combined_df, attributes)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_stats(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    stat_table = soup.findAll('table')[0].contents #Contents of the main table in html\n",
    "    \n",
    "    table_data = stat_table[3] #first 2 indices are empty strings, table_data is html starting from first table row\n",
    "    detail_data = table_data.find_all('p') #within table rows, there are <p> labels for table text\n",
    "    auxiliary_data = get_fight_auxiliary(soup)\n",
    "    \n",
    "    image_data = table_data.find_all('img') #get image links to find belt for \n",
    "    \n",
    "    contents = []\n",
    "    title_match_index = []\n",
    "    \n",
    "    #Loop through elements of detail_data (html table) to scrape fight details:\n",
    "    for index, item in enumerate(detail_data):\n",
    "        image = item.find('img')\n",
    "        if find_belt(image):\n",
    "            title_match_index.append(index) #find image of belt == title_bout\n",
    "        contents.append(item.text) #contents is list of all text from each element of table  \n",
    "\n",
    "    \n",
    "    #Clean up elements\n",
    "    contents = list(map(lambda x: remove_space_lines(x), contents))\n",
    "    contents = list(map(lambda x: x.strip(), contents)) \n",
    "    \n",
    "    draw_index = []\n",
    "    \n",
    "    #When there's a draw or NC, additional tags are created --> remove the tag to reformat correctly   \n",
    "    for i in np.arange(0, len(contents)-10, 16):\n",
    "\n",
    "        if contents[i] != 'win':\n",
    "            draw_index.append(np.floor_divide(i, 16))\n",
    "            contents.pop(i)\n",
    "        \n",
    "                    \n",
    "    #Extract links to more detailed fight statistics\n",
    "    fight_links = table_data.find_all('a', {'class': 'b-flag b-flag_style_green'})\n",
    "    fight_links = [item['href'] for item in fight_links]\n",
    "    \n",
    "    draw_links = table_data.find_all('a', {'class': 'b-flag b-flag_style_bordered'})\n",
    "    draw_links = [item['href'] for item in draw_links]\n",
    "    draw_links = list(dict.fromkeys(draw_links))\n",
    "    \n",
    "    for index, link in zip(draw_index, draw_links):\n",
    "        fight_links.insert(index, link)\n",
    "    \n",
    "    #each row of data is 16 elements: reformats 1 observation per row\n",
    "    formatted_contents = np.array(contents).reshape((-1, 16))\n",
    "    formatted_contents = pd.DataFrame(formatted_contents)\n",
    "    \n",
    "    #the first row is a list of 'wins'\n",
    "    #formatted_contents.drop(0, axis = 1, inplace = True)\n",
    "    \n",
    "    #Run a floor_divide to put the image of the belt in the correct fight\n",
    "\n",
    "    title_match = np.floor_divide(title_match_index, 16) \n",
    "\n",
    "    #Initialize title_bout column with all 0's\n",
    "    titles = np.zeros(16)\n",
    "    if len(title_match) != 0:\n",
    "        titles[title_match] = 1\n",
    "    \n",
    "    title_series = pd.Series(titles)\n",
    "    \n",
    "    formatted_contents['title_bout'] = title_series\n",
    "    \n",
    "    #rename columns\n",
    "    formatted_contents.columns = ['Winner', 'R_fighter', 'B_fighter', 'R_STR', 'B_STR', \n",
    "                               'R_TD', 'B_TD', 'R_SUB', 'R_SUB', 'R_PASS', 'B_PASS',\n",
    "                              'WEIGHT_CLASS', 'METHOD', 'DETAIL', 'ROUND', 'TIME', 'title_bout']\n",
    "    \n",
    "    #convert columns to appropriate data types\n",
    "    formatted_contents = formatted_contents.astype(data_types)\n",
    "    formatted_contents['TIME'] = formatted_contents['TIME'].apply(lambda x: dt.strptime(x, '%M:%S').time())\n",
    "    formatted_contents['link'] = fight_links\n",
    "    \n",
    "    return (formatted_contents, auxiliary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = all_event_urls[21]\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(requests.get(link).content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\s\\s+(.*)', 'Date:              September 07, 2019')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fight_auxiliary(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_page_stats(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fight_details(object):\n",
    "    def __init__(self, url):\n",
    "        self.all_details = get_detailed_page_stats(url)\n",
    "        self.fight_df = self.all_details[0]\n",
    "        self.attributes = self.all_details[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = fight_details(test_links[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_page1 = get_page_stats(detail_urls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_page1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_links = test_page1.link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for item in test_links:\n",
    "    a.append(fight_details(item))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1].attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link1 = page1.link[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_detailed_page_stats(link1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
